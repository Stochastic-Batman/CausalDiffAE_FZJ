1. Hello, I am Lado Turmanidze, an intern from Kutaisi International University, and for the past 6 weeks I’ve been working on structural causal models, diffusion models, and their combination - Causal Diffusion Autoencoders.

2. Combining the two makes sense, as integrating structural causal models (SCMs) with diffusion models aligns generative factors with causal variables, making them more interpretable and controllable, which can be crucial in some domains. SCMs enable explicit interventions, allowing users to generate and analyze counterfactual scenarios, and this is a huge advantage in fields with scarce data. 

3. Traditional machine learning models often identify correlations between variables, but they do not inherently provide insights into whether one variable causes changes in another. By representing the variables involved within a directed acyclic graph (DAG), one can identify potential confounders and assess the direct effects of medication on recovery. That is exactly what SCMs do. SCMs enable the analysis of counterfactual scenarios, which can answer questions of the form "What would have happened if...?".

4. There are certain assumptions that we must make to reason about SCMs. These assumptions are faithfulness (which basically ensures that the graph structure captures all and only the conditional independencies present in the distribution), no feedback loops, no conditioning on unobserved colliders, and no unobserved confounding, though the latter can be relaxed in some algorithms like FCI. The pseudocode you see is for the classic PC algorithm, which I had to formalize, as the original paper barely used math and described the whole algorithm in natural language.

5. SCMs consist of some d number of structural assignments, where each node is assigned a function that depends on the set of parents of this node - which can be an empty set—and a noise variable. We intervene on nodes using do-calculus, allowing us to overwrite the value of the node. Here is an example of a structural causal model, where X only depends on its noise, Z depends on X (its parent) and its noise, and Y depends on X, Z, and its noise. We can intervene with do(Z := k) to completely change the value of Z. We can also see how this affects Y, but not X.

6. Now, the second main concept is diffusion, which is an improvement over variational autoencoders. Here, instead of a single-step process, we construct a chain of transformations. We replace the encoder-decoder with forward and backward processes repeatedly. At each step, the transition distribution depends only on the immediately previous state, allowing the overall generation process to be decomposed into many simpler sub-tasks.

7. These are the pseudocodes for training and inference of Denoising Diffusion Probabilistic Models (DDPMs).

8. A key drawback of DDPMs is their reliance on many iterations to generate high-fidelity samples. For example, a DDPM would take more than 1000 hours to generate 50K 256x256 images on a standard GPU. This inefficiency arises because the reverse diffusion process requires iterative denoising. If the process intrinsically needs many steps to converge, generation will inherently be slow. Therefore, to accelerate computation, it is necessary to reduce the number of iterations.

9. Last year, a paper was published about Causal Diffusion Autoencoders - CausalDiffAE for short. The authors aim to model causal relations among semantic latent variables in order to learn causal representations and enable counterfactual generation in DPMs. The main idea is to learn a causal representation using a stochastic encoder and to model the relationships among latent variables by parameterizing causal mechanisms with neural networks. Decoding and stochastic variation modeling are achieved using a conditional DDIM. This approach yields a compact, causally relevant latent representation for reverse diffusion image synthesis. By explicitly modeling causal relations in the latent space, counterfactual samples can be generated via interventions on the learned causal variables. 

10. These are the loss function, training, and counterfactual generation pseudocodes for CausalDiffAE.

11. After I finished understanding the theory, I returned to the GitHub repository of the paper’s author, only to see that the issue I had opened was not answered at all. There are 3 examples that the code is supposed to test: one is a simple pendulum, where you have a light source and pendulum and want to see how the shadow changes; another is about the CausalCircuit dataset from Qualcomm, which is used for robotics; and the last one is the MorphoMNIST dataset, which is basically the MNIST dataset with morphometrics. I had to choose between them, as even the dataset generation did not work from the original author’s code, and there was no documentation, nor was it clear in README.md how to generate data. I chose the latter for its widespread use, as almost everyone knows about the MNIST dataset.

12. As the issue was not answered weeks after it was opened, I had to fork it and correct, improve, and document the changes. The image you see right now is all there is from the original repository, and it simply does not work. The original source code from the author has no instructions regarding data generation, lots of bugs, and redundant code. At one point, I thought it was better to just write the entire code from scratch... but Dr. Cao recommended trying to fix it, and it worked more and more, bit by bit. 

13. My fork has a clear README.md that transitions from the original source code to working code (for MorphoMNIST), a detailed walkthrough of every step - including data generation, code replacements, and parameter correction - and, except for data generation, if you clone the fork, you have everything already available.

14. Now, for the MorphoMNIST dataset, we experiment with thickness and intensity, where thickness T influences intensity I, but not vice versa. The stats are from a model trained for 210 400 steps with a batch size of 128. The model is evaluated with disentanglement and completeness scores, which are mathematically described in my research paper. Intuitively, the disentanglement score measures how well each latent variable captures an individual generative factor, and completeness quantifies to what extent each generative factor is predominantly explained by a single latent variable. The disentanglement score of 0.99 means the representation is highly interpretable, with minimal mixing between factors of variation. The completeness score of 0.53, however, reveals that while each latent cleanly maps to one factor, the reverse is less true - information about a single factor is distributed across several latents. In our case, the dependency from thickness to intensity likely contributes to this, as correlated factors are harder to isolate into individual latents.

15. As for the experiments, the model has been tested with 3 values of guidance weight w, namely 0, 0.5, and 1. w and 1-w are convex coefficients of conditional and unconditional model predictions, respectively. Due to the sampling technique, the original images on which we intervene differ for all three values of w. Experiments show that for w = 0, thickness intervention produces uniformly thick digits. Intensity intervention produces faint digits with varying thickness. 

16. For w = 0.5, thicker digits correlate with darker intensity, and fainter digits maintain thickness variations. 

17. w = 1 is an edge case, where extreme conditioning produces degenerate outputs.

18. So we conclude that the model correctly enforces "thickness implies intensity" and not the other way around.

Thanks for your attention!